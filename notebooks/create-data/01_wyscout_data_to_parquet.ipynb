{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the Wyscout data and turns them into parquet files. These are extremely fast to load so good for this prototyping kind of analysis.\n",
    "\n",
    "##### References:\n",
    "Pappalardo, Luca; Massucco, Emanuele (2019): Soccer match event dataset. figshare. Collection. https://doi.org/10.6084/m9.figshare.c.4415000\n",
    "\n",
    "Pappalardo, L., Cintia, P., Rossi, A. et al. A public data set of spatio-temporal match events in soccer competitions. Sci Data 6, 236 (2019). https://doi.org/10.1038/s41597-019-0247-7\n",
    "\n",
    "Data link: https://figshare.com/collections/Soccer_match_event_dataset/4415000/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from mplsoccer.statsbomb import _split_location_cols, _split_dict_col, _list_dictionary_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these paths/ parameters\n",
    "You will need to change these paths/ parameters depending on where the StatsBomb open-data is located, how and where you want to save the resulting data, and if you only want the new files to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files in folder in current directory. To change if want to save elsewhere\n",
    "DATA_FOLDER = os.path.join('..', '..', 'data', 'wyscout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that are jsons\n",
    "JSON_LINKS = ['https://ndownloader.figshare.com/files/15073868',  # coaches\n",
    "              # 'https://ndownloader.figshare.com/files/15074030',  # referees <- not downloaded as corrupt\n",
    "              'https://ndownloader.figshare.com/files/15073721',  # players\n",
    "              'https://ndownloader.figshare.com/files/15073697',  # teams\n",
    "              'https://ndownloader.figshare.com/files/15073685',  # competitions\n",
    "              'https://raw.githubusercontent.com/andrewRowlinson/mplsoccer/master/wyscout_event_tags.json',  # my decode tags\n",
    "              ]  # competitions\n",
    "JSON_FILES = ['coach.json', \n",
    "              #'referees.json',  # <- not downloaded as corrupt\n",
    "              'player.json', 'team.json', 'competition.json',\n",
    "              'event_tag.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that are zipped\n",
    "ZIP_LINKS = ['https://ndownloader.figshare.com/files/14464685',  # events\n",
    "             'https://ndownloader.figshare.com/files/14464622']  # matches\n",
    "ZIP_FILES = ['events.zip', 'matches.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure\n",
    "for folder in ['json', 'event_raw', 'match_raw']:\n",
    "    path = os.path.join(DATA_FOLDER, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128, json=False):\n",
    "    '''Souce: https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url '''\n",
    "    r = requests.get(url, stream=True)\n",
    "    if json:\n",
    "        r.encoding = 'unicode-escape'\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download json files\n",
    "for i, link in enumerate(JSON_LINKS):\n",
    "    download_url(link, os.path.join(DATA_FOLDER, 'json', JSON_FILES[i]), json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download zip files, extract jsons, and remove original zip files\n",
    "for i, link in enumerate(ZIP_LINKS):\n",
    "    save_path = os.path.join(DATA_FOLDER, 'json', ZIP_FILES[i])\n",
    "    download_url(link, save_path)\n",
    "    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(DATA_FOLDER, 'json'))\n",
    "    os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coach = pd.read_json(os.path.join(DATA_FOLDER, 'json', 'coach.json'), encoding='unicode-escape')\n",
    "for col in ['passportArea', 'birthArea']:\n",
    "    df_coach = _split_dict_col(df_coach, col)\n",
    "df_coach.to_parquet(os.path.join(DATA_FOLDER, 'coach.parquet'))\n",
    "df_coach.rename({'wyId': 'coach_id'}, axis=1, inplace=True)\n",
    "df_coach.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player = pd.read_json(os.path.join(DATA_FOLDER, 'json', 'player.json'), encoding='unicode-escape')\n",
    "for col in ['passportArea', 'role', 'birthArea']:\n",
    "    df_player = _split_dict_col(df_player, col)\n",
    "# some of the ids are null some are 'null' as text :)\n",
    "for col in ['currentTeamId', 'currentNationalTeamId', 'passportArea_id', 'birthArea_id']:\n",
    "    mask_null = (df_player[col].isnull())|(df_player[col] == 'null')\n",
    "    df_player.loc[mask_null, col] = np.nan\n",
    "    df_player[col] = df_player[col].astype(np.float32)\n",
    "df_player.rename({'wyId': 'player_id'}, axis=1, inplace=True)\n",
    "df_player.to_parquet(os.path.join(DATA_FOLDER, 'player.parquet'))\n",
    "df_player.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team = pd.read_json(os.path.join(DATA_FOLDER, 'json', 'team.json'), encoding='unicode-escape')\n",
    "df_team = _split_dict_col(df_team, 'area')\n",
    "df_team['area_id'] = df_team.area_id.astype(np.int32)\n",
    "df_team.to_parquet(os.path.join(DATA_FOLDER, 'team.parquet'))\n",
    "df_team.rename({'wyId': 'team_id'}, axis=1, inplace=True)\n",
    "team_rename = {'Real Club Celta de Vigo': 'Celta Vigo',\n",
    "               'Valencia Club de Fútbol': 'Valencia',\n",
    "               'FC Barcelona': 'Barcelona',\n",
    "               'Real Betis Balompié': 'Real Betis',\n",
    "               'Girona FC': 'Girona',\n",
    "               'CD Leganés': 'Leganés',\n",
    "               'Real Sociedad de Fútbol': 'Real Sociedad',\n",
    "               'Real Club Deportivo de La Coruña': 'Deportivo La Coruna',\n",
    "               'Sevilla FC': 'Sevilla',\n",
    "               'Getafe Club de Fútbol': 'Getafe',\n",
    "               'Athletic Club Bilbao': 'Athletic Bilbao',\n",
    "               'Real Madrid Club de Fútbol': 'Real Madrid',\n",
    "               'Málaga Club de Fútbol': 'Málaga',\n",
    "               'Levante UD': 'Levante',\n",
    "               'Reial Club Deportiu Espanyol': 'Espanyol',\n",
    "               'UD Las Palmas': 'Las Palmas',\n",
    "               'SD Eibar': 'Eibar',\n",
    "               'Villarreal Club de Fútbol': 'Villarreal',\n",
    "               'Club Atlético de Madrid': 'Atlético Madrid',\n",
    "               'Korea Republic': 'South Korea'}\n",
    "df_team.officialName.replace(team_rename, inplace=True)\n",
    "df_team.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competition = pd.read_json(os.path.join(DATA_FOLDER, 'json', 'competition.json'), encoding='unicode-escape')\n",
    "df_competition = _split_dict_col(df_competition, 'area')\n",
    "# if the area id is '0' as text for internationals set to missing\n",
    "df_competition.loc[df_competition.format=='International cup', 'area_id'] = np.nan\n",
    "df_competition['area_id'] = df_competition.area_id.astype(np.float32)\n",
    "# make same format as StatsBomb: competition_country_name\n",
    "mask = df_competition.type=='club'\n",
    "df_competition.loc[mask, 'competition_country_name'] = df_competition.loc[mask, 'area_name']\n",
    "mask = df_competition.type=='international'\n",
    "df_competition.loc[mask, 'competition_country_name'] = 'International'\n",
    "# add gender\n",
    "df_competition['competition_gender'] = 'male'\n",
    "# replace with competition real names\n",
    "df_competition.name.replace({'Spanish first division': 'La Liga',\n",
    "                             'World Cup': 'FIFA World Cup',\n",
    "                             'Italian first division': 'Serie A',\n",
    "                             'English first division': 'Premier League',\n",
    "                             'French first division': 'Ligue 1',\n",
    "                             'German first division': 'Bundesliga',\n",
    "                             'European Championship': 'UEFA Euro'}, inplace=True)\n",
    "# rename competition name\n",
    "df_competition.rename({'name': 'competition_name', 'wyId': 'competition_id'}, axis=1, inplace=True)\n",
    "# add season name\n",
    "df_competition.loc[df_competition.type == 'club', 'season_name'] = '2017/2018'\n",
    "df_competition.loc[df_competition.competition_name == 'UEFA Euro', 'season_name'] = '2016'\n",
    "df_competition.loc[df_competition.competition_name == 'FIFA World Cup', 'season_name'] = '2018'\n",
    "df_competition.to_parquet(os.path.join(DATA_FOLDER, 'competition.parquet'))\n",
    "df_competition.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not interested in formations or lineups so did not parse them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of match files\n",
    "match_list = glob.glob(os.path.join(DATA_FOLDER, 'json', 'matches*.json'))\n",
    "\n",
    "# loop through match files as save as seperate parquet files\n",
    "for file in match_list:\n",
    "    \n",
    "    # match dataframe\n",
    "    df_match = pd.read_json(file, encoding='unicode-escape')\n",
    "    \n",
    "    # split the team information from the teamsData column into two seperate columns\n",
    "    col = 'teamsData'\n",
    "    df_match[col] = df_match[col].apply(lambda x: {} if pd.isna(x) else x)\n",
    "    df_match['team1'] = df_match.teamsData.apply(lambda x: x[list(x.keys())[0]])\n",
    "    df_match['team2'] = df_match.teamsData.apply(lambda x: x[list(x.keys())[1]])\n",
    "    \n",
    "    # split team information stored as a dictionary into seperate columns\n",
    "    df_match = _split_dict_col(df_match, 'team1')\n",
    "    df_match = _split_dict_col(df_match, 'team2')\n",
    "    \n",
    "    # add home and away teams and scores up to extra time\n",
    "    mask = df_match.team1_side == 'home'\n",
    "    mask_et = (df_match.team1_scoreET > 0) | (df_match.team2_scoreET > 0)\n",
    "    df_match.loc[mask,'home_score'] = df_match.loc[mask,'team1_score']\n",
    "    df_match.loc[mask,'away_score'] = df_match.loc[mask,'team2_score']\n",
    "    df_match.loc[~mask,'home_score'] = df_match.loc[~mask,'team2_score']\n",
    "    df_match.loc[~mask,'away_score'] = df_match.loc[~mask,'team1_score']\n",
    "    df_match.loc[mask_et & mask,'home_score'] = df_match.loc[mask_et & mask,'team1_scoreET']\n",
    "    df_match.loc[mask_et & mask,'away_score'] = df_match.loc[mask_et & mask,'team2_scoreET']\n",
    "    df_match.loc[mask_et & ~mask,'home_score'] = df_match.loc[mask_et & ~mask,'team2_scoreET']\n",
    "    df_match.loc[mask_et & ~mask,'away_score'] = df_match.loc[mask_et & ~mask,'team1_scoreET']    \n",
    "    \n",
    "    # add away/ home team info\n",
    "    df_match.loc[mask, 'home_team_id'] = df_match.loc[mask, 'team1_teamId']\n",
    "    df_match.loc[~mask, 'home_team_id'] = df_match.loc[~mask, 'team2_teamId']\n",
    "    df_match.loc[mask, 'away_team_id'] = df_match.loc[mask, 'team2_teamId']\n",
    "    df_match.loc[~mask, 'away_team_id'] = df_match.loc[~mask, 'team1_teamId']\n",
    "    \n",
    "    # add away/home coach info\n",
    "    df_match.loc[mask, 'home_team_coach_id'] = df_match.loc[mask, 'team1_coachId']\n",
    "    df_match.loc[~mask, 'home_team_coach_id'] = df_match.loc[~mask, 'team2_coachId']\n",
    "    df_match.loc[mask, 'away_team_coach_id'] = df_match.loc[mask, 'team2_coachId']\n",
    "    df_match.loc[~mask, 'away_team_coach_id'] = df_match.loc[~mask, 'team1_coachId']\n",
    "\n",
    "    # format date columns\n",
    "    df_match['dateutc'] = pd.to_datetime(df_match.dateutc)\n",
    "    df_match['kick_off'] = pd.to_datetime(df_match.date.astype(str).str[:-6])\n",
    "    \n",
    "    # rename columns\n",
    "    df_match.rename({'wyId': 'match_id',\n",
    "                     'gameweek': 'match_week',\n",
    "                     'seasonId': 'season_id',\n",
    "                     'competitionId': 'competition_id',\n",
    "                     'venue': 'stadium_name'}, axis=1, inplace=True)\n",
    "    \n",
    "    # add competition info\n",
    "    df_match = df_match.merge(df_competition[['competition_id',\n",
    "                                              'competition_country_name',\n",
    "                                              'competition_name',\n",
    "                                              'season_name',\n",
    "                                              'competition_gender']], on='competition_id', how='left')\n",
    "    \n",
    "    # add team info\n",
    "    df_match = df_match.merge(df_team[['team_id', 'officialName']],\n",
    "                              left_on='home_team_id', right_on='team_id', how='left')\n",
    "    df_match = df_match.merge(df_team[['team_id', 'officialName']],\n",
    "                              left_on='away_team_id', right_on='team_id', how='left', suffixes=['_home', '_away'])\n",
    "    \n",
    "    df_match.rename({'officialName_home': 'home_team_name',\n",
    "                     'officialName_away': 'away_team_name'}, axis=1, inplace=True)\n",
    "    \n",
    "    # drop columns\n",
    "    df_match.drop(['date', 'status', 'winner', 'referees', 'team_id_away', 'team_id_home',\n",
    "                   'team1_formation_bench', 'team1_formation_lineup', 'team1_formation_substitutions',\n",
    "                   'team2_formation_bench', 'team2_formation_lineup', 'team2_formation_substitutions',\n",
    "                   'team1_hasFormation', 'team2_hasFormation',\n",
    "                   'team1_score', 'team1_scoreP', 'team1_scoreHT', 'team1_scoreET',\n",
    "                   'team2_score', 'team2_scoreP', 'team2_scoreHT', 'team2_scoreET',\n",
    "                   'teamsData', 'team1_teamId', 'team2_teamId', 'team2_side',\n",
    "                   'team1_side', 'team1_coachId', 'team2_coachId'], axis=1, inplace=True)\n",
    "    \n",
    "    save_path = os.path.join(DATA_FOLDER, 'match_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_match.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get matches as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_files = glob.glob(os.path.join(DATA_FOLDER, 'match_raw', '*.parquet'))\n",
    "df_match = pd.concat([pd.read_parquet(file) for file in match_files])\n",
    "df_match.to_parquet(os.path.join(DATA_FOLDER, 'match.parquet'))\n",
    "df_match.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of event files\n",
    "events_list = glob.glob(os.path.join(DATA_FOLDER, 'json', 'events*.json'))\n",
    "\n",
    "# loop through event files as save as seperate parquet files\n",
    "for file in events_list:\n",
    "    \n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    # load as dataframe\n",
    "    df_event = pd.read_json(file, encoding='unicode-escape')\n",
    "    \n",
    "    # split start and end positions\n",
    "    _split_location_cols(df_event, 'positions', ['start', 'end'])\n",
    "    \n",
    "    # create seperate columns for the x/y coordinates\n",
    "    for col in ['start', 'end']:\n",
    "        df_event = _split_dict_col(df_event, col)\n",
    "        \n",
    "    # set dodgy end coordinates to null\n",
    "    mask = df_event.eventName.isin(['Shot', 'Interruption', 'Offside'])\n",
    "    mask2 = df_event.subEventName.isin(['Free kick shot', 'Hand foul', 'Late card foul', 'Out of game foul', 'Protest',\n",
    "                                        'Simulation', 'Time lost foul', 'Violent Foul'])\n",
    "    df_event.loc[mask | mask2, 'end_x'] = np.nan\n",
    "    df_event.loc[mask | mask2, 'end_y'] = np.nan\n",
    "    \n",
    "    # wyscout has some dodgy end_y/ end_x near the corners. Convert to np.nan\n",
    "    mask_dodgy_end = (((df_event.end_y == 100) & (df_event.end_x == 100)) | \n",
    "                      ((df_event.end_x == 0) & (df_event.end_y == 0)))\n",
    "    df_event.loc[mask_dodgy_end, 'end_y'] = np.nan\n",
    "    df_event.loc[mask_dodgy_end, 'end_x'] = np.nan\n",
    "    \n",
    "    # set dodgy start coordinates to null\n",
    "    df_event.loc[df_event.eventName.isin(['Save attempt', 'Goalkeeper leaving line']), 'start_x'] = np.nan\n",
    "    df_event.loc[df_event.eventName.isin(['Save attempt', 'Goalkeeper leaving line']), 'start_y'] = np.nan\n",
    "    \n",
    "    # fix start coordinates for goal kicks\n",
    "    df_event.loc[df_event.subEventName == 'Goal kick', 'start_x'] = 6.\n",
    "    df_event.loc[df_event.subEventName == 'Goal kick', 'start_y'] = 50.\n",
    "    \n",
    "    # create a seperate column for each tag in the dictionary\n",
    "    df_new = pd.DataFrame(df_event['tags'].tolist(), index=df_event.index)\n",
    "    for tag in df_new.columns:\n",
    "        df_new.loc[df_new[tag].notnull(), tag] = df_new.loc[df_new[tag].notnull(), tag].apply(lambda x: x['id'])\n",
    "        \n",
    "    # summarise tag id columns into boolean columns for each tag and a string column for position \n",
    "    cols_to_drop = df_new.columns\n",
    "    df_tag = pd.read_json(os.path.join(DATA_FOLDER, 'json', 'event_tag.json'))\n",
    "    position_tags = df_tag.loc[df_tag.tag_name.str[:8] == 'position', 'tag_id'].values\n",
    "    for i, row in df_tag.iterrows():\n",
    "        if row['tag_id'] not in position_tags:\n",
    "            df_new.loc[(df_new==row['tag_id']).any(axis=1), row['tag_name']] = True\n",
    "        else:\n",
    "            df_new.loc[(df_new==row['tag_id']).any(axis=1), 'position'] = row['tag_name']\n",
    "            \n",
    "    # remove 'position' and '_' from text in the position column\n",
    "    df_new['position'] = df_new.position.str[9:].str.replace('_', ' ')\n",
    "    df_new.loc[df_new['position'].isnull(), 'position'] = None\n",
    "    \n",
    "    # replace missing with False for boolean columns\n",
    "    other_tags = df_tag.loc[df_tag.tag_name.str[:8] != 'position', 'tag_name'].values\n",
    "    df_new[other_tags] = df_new[other_tags].replace({np.nan: False})\n",
    "    \n",
    "    # drop tag id columns\n",
    "    df_new.drop(cols_to_drop, axis=1, inplace=True)                                               \n",
    "                                        \n",
    "    # add tags to the dataset\n",
    "    df_event = pd.concat([df_event, df_new], axis=1)\n",
    "    \n",
    "    # drop tag column\n",
    "    df_event.drop('tags', axis=1, inplace=True)\n",
    "    \n",
    "    # deal with blank subEventId\n",
    "    df_event.loc[df_event.subEventId=='', 'subEventId'] = None\n",
    "    df_event['subEventId'] = df_event['subEventId'].astype(np.float32)\n",
    "    \n",
    "    # rename columns for consistency with other datasets\n",
    "    df_event.rename({'playerId': 'player_id',\n",
    "                     'start_y': 'y',\n",
    "                     'start_x': 'x',\n",
    "                     'matchId': 'match_id',\n",
    "                     'teamId': 'team_id',}, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # save to parquet\n",
    "    save_path = os.path.join(DATA_FOLDER, 'event_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_event.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get events as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = glob.glob(os.path.join(DATA_FOLDER, 'event_raw', '*.parquet'))\n",
    "df_event = pd.concat([pd.read_parquet(file) for file in event_files])\n",
    "df_event.sort_values(['match_id', 'matchPeriod', 'eventSec'], inplace=True)\n",
    "df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "df_event.info(verbose=True, null_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
