{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the Wyscout data and turns them into parquet files. These are extremely fast to load so good for this prototyping kind of analysis.\n",
    "\n",
    "##### References:\n",
    "Pappalardo, Luca; Massucco, Emanuele (2019): Soccer match event dataset. figshare. Collection. https://doi.org/10.6084/m9.figshare.c.4415000\n",
    "\n",
    "Pappalardo, L., Cintia, P., Rossi, A. et al. A public data set of spatio-temporal match events in soccer competitions. Sci Data 6, 236 (2019). https://doi.org/10.1038/s41597-019-0247-7\n",
    "\n",
    "\n",
    "This updated code uses version 5 of this dataset: https://figshare.com/collections/Soccer_match_event_dataset/4415000/5\n",
    "\n",
    "The dataframes have the following number of entries:\n",
    "\n",
    "- df_coach: 208 entries\n",
    "- df_player: 3,603 entries\n",
    "- df_team: 142 entries\n",
    "- df_competition: 7 entries\n",
    "- df_match: 1,941 entries\n",
    "- df_formation: 74,098 entries\n",
    "- df_substitution: 11,097 entries\n",
    "- df_event: 3,251,294 entries\n",
    "\n",
    "The original code used version 2 of this dataset: https://figshare.com/collections/Soccer_match_event_dataset/4415000/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for wrangling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location_cols(df, col, new_cols):\n",
    "    \"\"\" Location is stored as a list. split into columns. \"\"\"\n",
    "    for new_col in new_cols:\n",
    "        df[new_col] = np.nan\n",
    "    if col in df.columns:\n",
    "        mask_not_null = df[col].notnull()\n",
    "        df_not_null = df.loc[mask_not_null, col]\n",
    "        df_new = pd.DataFrame(df_not_null.tolist(), index=df_not_null.index)\n",
    "        new_cols = new_cols[:len(df_new.columns)]  # variable whether z location is present\n",
    "        df_new.columns = new_cols\n",
    "        df.loc[mask_not_null, new_cols] = df_new\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "def split_dict_col(df, col):\n",
    "    \"\"\" Function to split a dictionary column to separate columns.\"\"\"\n",
    "    # handle missing data by filling with an empty dictionary\n",
    "    df[col] = df[col].apply(lambda x: {} if pd.isna(x) else x)\n",
    "    # split the non-missing data and change the column names\n",
    "    df_temp_cols = pd.json_normalize(df[col]).set_index(df.index)\n",
    "    col_names = df_temp_cols.columns\n",
    "    # note add column description to column name if doesn't already contain it\n",
    "    col_names = [c.replace('.', '_') if c[:len(col)] == col else\n",
    "                 (col+'_'+c).replace('.', '_') for c in col_names]\n",
    "    df[col_names] = df_temp_cols\n",
    "    # drop old column\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def list_dictionary_to_df(df, col, value_name, var_name, id_col='id'):\n",
    "    \"\"\" Some columns are a list of dictionaries. This turns them into a new dataframe of rows.\"\"\"\n",
    "    df = df.loc[df[col].notnull(), [id_col, col]]\n",
    "    df.set_index(id_col, inplace=True)\n",
    "    df = df[col].apply(pd.Series).copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.melt(id_vars=id_col, value_name=value_name, var_name=var_name)\n",
    "    df[var_name] = df[var_name] + 1\n",
    "    df = df[df[value_name].notnull()].copy()\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these paths/ parameters\n",
    "You will need to change these paths/ parameters depending on where the StatsBomb open-data is located, how and where you want to save the resulting data, and if you only want the new files to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files in folder in current directory. To change if want to save elsewhere\n",
    "DATA_FOLDER = os.path.join('..', '..', 'data', 'wyscout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that are jsons/csv\n",
    "COACHES_V5 = 'https://ndownloader.figshare.com/files/15073868'  # json\n",
    "REFEREES_V5 = 'https://ndownloader.figshare.com/files/15074030'  # json\n",
    "PLAYERS_V5 = 'https://ndownloader.figshare.com/files/15073721'  # json\n",
    "TEAMS_V5 = 'https://ndownloader.figshare.com/files/15073697'  # json\n",
    "COMPETITION_V5 = 'https://ndownloader.figshare.com/files/15073685'  # json\n",
    "EVENT_ID_MAPS_V5 = 'https://ndownloader.figshare.com/files/21385245'  # csv\n",
    "EVENT_TAG_MAPS_V5 = 'https://ndownloader.figshare.com/files/21385239'  # csv\n",
    "\n",
    "JSON_CSV_LINKS = [COACHES_V5, REFEREES_V5, PLAYERS_V5, TEAMS_V5, COMPETITION_V5, EVENT_ID_MAPS_V5, EVENT_TAG_MAPS_V5]\n",
    "JSON_CSV_FILES = ['coach.json', 'referees.json', 'player.json', 'team.json', 'competition.json',\n",
    "                  'eventid2name.csv', 'tags2name.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that are zipped\n",
    "EVENTS_V5 = 'https://ndownloader.figshare.com/files/14464685'\n",
    "MATCHES_V5 = 'https://ndownloader.figshare.com/files/14464622'\n",
    "\n",
    "ZIP_LINKS = [EVENTS_V5, MATCHES_V5]\n",
    "ZIP_FILES = ['events.zip', 'matches.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure\n",
    "for folder in ['raw', 'event_raw', 'match_raw', 'formation_raw', 'substitution_raw']:\n",
    "    path = os.path.join(DATA_FOLDER, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128, json=False):\n",
    "    '''Souce: https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url '''\n",
    "    r = requests.get(url, stream=True)\n",
    "    if json:\n",
    "        r.encoding = 'unicode-escape'\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download json files\n",
    "for i, link in enumerate(JSON_CSV_LINKS):\n",
    "    download_url(link, os.path.join(DATA_FOLDER, 'raw', JSON_CSV_FILES[i]), json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download zip files, extract jsons, and remove original zip files\n",
    "for i, link in enumerate(ZIP_LINKS):\n",
    "    save_path = os.path.join(DATA_FOLDER, 'raw', ZIP_FILES[i])\n",
    "    download_url(link, save_path)\n",
    "    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(DATA_FOLDER, 'raw'))\n",
    "    os.remove(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename dictionary for consistency with StatsBomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_rename = {'Real Club Celta de Vigo': 'Celta Vigo',\n",
    "               'Valencia Club de Fútbol': 'Valencia',\n",
    "               'FC Barcelona': 'Barcelona',\n",
    "               'Real Betis Balompié': 'Real Betis',\n",
    "               'Girona FC': 'Girona',\n",
    "               'CD Leganés': 'Leganés',\n",
    "               'Real Sociedad de Fútbol': 'Real Sociedad',\n",
    "               'Real Club Deportivo de La Coruña': 'Deportivo La Coruna',\n",
    "               'Sevilla FC': 'Sevilla',\n",
    "               'Getafe Club de Fútbol': 'Getafe',\n",
    "               'Athletic Club Bilbao': 'Athletic Club',\n",
    "               'Real Madrid Club de Fútbol': 'Real Madrid',\n",
    "               'Málaga Club de Fútbol': 'Málaga',\n",
    "               'Levante UD': 'Levante',\n",
    "               'Reial Club Deportiu Espanyol': 'Espanyol',\n",
    "               'UD Las Palmas': 'Las Palmas',\n",
    "               'SD Eibar': 'Eibar',\n",
    "               'Villarreal Club de Fútbol': 'Villarreal',\n",
    "               'Manchester United FC': 'Manchester United',\n",
    "               'Manchester City FC': 'Manchester City',\n",
    "               'Tottenham Hotspur FC': 'Tottenham Hotspur',\n",
    "               'AS Monaco FC': 'AS Monaco',\n",
    "               'Newcastle United FC': 'Newcastle United',\n",
    "               'Leicester City FC': 'Leicester City',\n",
    "               'Juventus FC': 'Juventus',\n",
    "               'BV Borussia 09 Dortmund': 'Borussia Dortmund',\n",
    "               'Everton FC': 'Everton',\n",
    "               'Arsenal FC': 'Arsenal',\n",
    "               'Southampton FC': 'Southampton',\n",
    "               'Liverpool FC': 'Liverpool',\n",
    "               'Chelsea FC': 'Chelsea',\n",
    "               'Club Atlético de Madrid': 'Atlético Madrid',\n",
    "               'Korea Republic': 'South Korea'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coach = pd.read_json(os.path.join(DATA_FOLDER, 'raw', 'coach.json'), encoding='unicode-escape')\n",
    "for col in ['passportArea', 'birthArea']:\n",
    "    df_coach = split_dict_col(df_coach, col)\n",
    "df_coach.to_parquet(os.path.join(DATA_FOLDER, 'coach.parquet'))\n",
    "df_coach.rename({'wyId': 'coach_id'}, axis=1, inplace=True)\n",
    "df_coach.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player = pd.read_json(os.path.join(DATA_FOLDER, 'raw', 'player.json'), encoding='unicode-escape')\n",
    "for col in ['passportArea', 'role', 'birthArea']:\n",
    "    df_player = split_dict_col(df_player, col)\n",
    "# some of the ids are null some are 'null' as text :)\n",
    "for col in ['currentTeamId', 'currentNationalTeamId', 'passportArea_id', 'birthArea_id']:\n",
    "    mask_null = (df_player[col].isnull())|(df_player[col] == 'null')\n",
    "    df_player.loc[mask_null, col] = np.nan\n",
    "    df_player[col] = df_player[col].astype(np.float32)\n",
    "df_player.rename({'wyId': 'player_id'}, axis=1, inplace=True)\n",
    "df_player.to_parquet(os.path.join(DATA_FOLDER, 'player.parquet'))\n",
    "df_player.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_team = pd.read_json(os.path.join(DATA_FOLDER, 'raw', 'team.json'), encoding='unicode-escape')\n",
    "df_team = split_dict_col(df_team, 'area')\n",
    "df_team['area_id'] = df_team.area_id.astype(np.int32)\n",
    "df_team.rename({'wyId': 'team_id'}, axis=1, inplace=True)\n",
    "df_team.officialName.replace(team_rename, inplace=True)\n",
    "df_team.to_parquet(os.path.join(DATA_FOLDER, 'team.parquet'))\n",
    "df_team.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competition = pd.read_json(os.path.join(DATA_FOLDER, 'raw', 'competition.json'), encoding='unicode-escape')\n",
    "df_competition = split_dict_col(df_competition, 'area')\n",
    "# if the area id is '0' as text for internationals set to missing\n",
    "df_competition.loc[df_competition.format=='International cup', 'area_id'] = np.nan\n",
    "df_competition['area_id'] = df_competition.area_id.astype(np.float32)\n",
    "# make same format as StatsBomb: competition_country_name\n",
    "mask = df_competition.type=='club'\n",
    "df_competition.loc[mask, 'competition_country_name'] = df_competition.loc[mask, 'area_name']\n",
    "mask = df_competition.type=='international'\n",
    "df_competition.loc[mask, 'competition_country_name'] = 'International'\n",
    "# add gender\n",
    "df_competition['competition_gender'] = 'male'\n",
    "# replace with competition real names\n",
    "df_competition.name.replace({'Spanish first division': 'La Liga',\n",
    "                             'World Cup': 'FIFA World Cup',\n",
    "                             'Italian first division': 'Serie A',\n",
    "                             'English first division': 'Premier League',\n",
    "                             'French first division': 'Ligue 1',\n",
    "                             'German first division': 'Bundesliga',\n",
    "                             'European Championship': 'UEFA Euro'}, inplace=True)\n",
    "# rename competition name\n",
    "df_competition.rename({'name': 'competition_name', 'wyId': 'competition_id'}, axis=1, inplace=True)\n",
    "# add season name\n",
    "df_competition.loc[df_competition.type == 'club', 'season_name'] = '2017/2018'\n",
    "df_competition.loc[df_competition.competition_name == 'UEFA Euro', 'season_name'] = '2016'\n",
    "df_competition.loc[df_competition.competition_name == 'FIFA World Cup', 'season_name'] = '2018'\n",
    "df_competition.to_parquet(os.path.join(DATA_FOLDER, 'competition.parquet'))\n",
    "df_competition.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of match files\n",
    "match_list = glob.glob(os.path.join(DATA_FOLDER, 'raw', 'matches*.json'))\n",
    "\n",
    "# loop through match files as save as seperate parquet files\n",
    "for file in match_list:\n",
    "    \n",
    "    # match dataframe\n",
    "    df_match = pd.read_json(file, encoding='unicode-escape')\n",
    "    \n",
    "    # split the team information from the teamsData column into two seperate columns\n",
    "    col = 'teamsData'\n",
    "    df_match[col] = df_match[col].apply(lambda x: {} if pd.isna(x) else x)\n",
    "    df_match['team1'] = df_match.teamsData.apply(lambda x: x[list(x.keys())[0]])\n",
    "    df_match['team2'] = df_match.teamsData.apply(lambda x: x[list(x.keys())[1]])\n",
    "    \n",
    "    # split team information stored as a dictionary into seperate columns\n",
    "    df_match = split_dict_col(df_match, 'team1')\n",
    "    df_match = split_dict_col(df_match, 'team2')\n",
    "    \n",
    "    # add home and away teams and scores up to extra time\n",
    "    mask = df_match.team1_side == 'home'\n",
    "    mask_et = (df_match.team1_scoreET > 0) | (df_match.team2_scoreET > 0)\n",
    "    df_match.loc[mask,'home_score'] = df_match.loc[mask,'team1_score']\n",
    "    df_match.loc[mask,'away_score'] = df_match.loc[mask,'team2_score']\n",
    "    df_match.loc[~mask,'home_score'] = df_match.loc[~mask,'team2_score']\n",
    "    df_match.loc[~mask,'away_score'] = df_match.loc[~mask,'team1_score']\n",
    "    df_match.loc[mask_et & mask,'home_score'] = df_match.loc[mask_et & mask,'team1_scoreET']\n",
    "    df_match.loc[mask_et & mask,'away_score'] = df_match.loc[mask_et & mask,'team2_scoreET']\n",
    "    df_match.loc[mask_et & ~mask,'home_score'] = df_match.loc[mask_et & ~mask,'team2_scoreET']\n",
    "    df_match.loc[mask_et & ~mask,'away_score'] = df_match.loc[mask_et & ~mask,'team1_scoreET']    \n",
    "    \n",
    "    # add away/ home team info\n",
    "    df_match.loc[mask, 'home_team_id'] = df_match.loc[mask, 'team1_teamId']\n",
    "    df_match.loc[~mask, 'home_team_id'] = df_match.loc[~mask, 'team2_teamId']\n",
    "    df_match.loc[mask, 'away_team_id'] = df_match.loc[mask, 'team2_teamId']\n",
    "    df_match.loc[~mask, 'away_team_id'] = df_match.loc[~mask, 'team1_teamId']\n",
    "    \n",
    "    # add away/home coach info\n",
    "    df_match.loc[mask, 'home_team_coach_id'] = df_match.loc[mask, 'team1_coachId']\n",
    "    df_match.loc[~mask, 'home_team_coach_id'] = df_match.loc[~mask, 'team2_coachId']\n",
    "    df_match.loc[mask, 'away_team_coach_id'] = df_match.loc[mask, 'team2_coachId']\n",
    "    df_match.loc[~mask, 'away_team_coach_id'] = df_match.loc[~mask, 'team1_coachId']\n",
    "\n",
    "    # format date columns\n",
    "    df_match['dateutc'] = pd.to_datetime(df_match.dateutc)\n",
    "    df_match['kick_off'] = pd.to_datetime(df_match.date.astype(str).str[:-6])\n",
    "    \n",
    "    # rename columns\n",
    "    df_match.rename({'wyId': 'match_id',\n",
    "                     'gameweek': 'match_week',\n",
    "                     'seasonId': 'season_id',\n",
    "                     'competitionId': 'competition_id',\n",
    "                     'venue': 'stadium_name'}, axis=1, inplace=True)\n",
    "    \n",
    "    # add competition info\n",
    "    df_match = df_match.merge(df_competition[['competition_id',\n",
    "                                              'competition_country_name',\n",
    "                                              'competition_name',\n",
    "                                              'season_name',\n",
    "                                              'competition_gender']], on='competition_id', how='left')\n",
    "    \n",
    "    # add team info\n",
    "    df_match = df_match.merge(df_team[['team_id', 'officialName']],\n",
    "                              left_on='home_team_id', right_on='team_id', how='left')\n",
    "    df_match = df_match.merge(df_team[['team_id', 'officialName']],\n",
    "                              left_on='away_team_id', right_on='team_id', how='left', suffixes=['_home', '_away'])\n",
    "    \n",
    "    df_match.rename({'officialName_home': 'home_team_name',\n",
    "                     'officialName_away': 'away_team_name'}, axis=1, inplace=True)\n",
    "    \n",
    "    # replace some team names to be the same as StatsBomb\n",
    "    df_match.home_team_name.replace(team_rename, inplace=True)\n",
    "    df_match.away_team_name.replace(team_rename, inplace=True)\n",
    "\n",
    "    # dataframes with the team id for adding to the formation/ substitutions\n",
    "    df_team1 = df_match[['match_id', 'team1_teamId']].rename({'team1_teamId': 'team_id'}, axis=1)\n",
    "    df_team2 = df_match[['match_id', 'team2_teamId']].rename({'team2_teamId': 'team_id'}, axis=1)\n",
    "    \n",
    "    # formation lineup dataframe\n",
    "    df_team1_formation_lineup = list_dictionary_to_df(df_match, 'team1_formation_lineup', 'lineup',\n",
    "                                                      'lineup_id', 'match_id')\n",
    "    df_team2_formation_lineup = list_dictionary_to_df(df_match, 'team2_formation_lineup', 'lineup',\n",
    "                                                      'lineup_id', 'match_id')\n",
    "    df_team1_formation_lineup = df_team1_formation_lineup.merge(df_team1, on='match_id', how='left')\n",
    "    df_team2_formation_lineup = df_team2_formation_lineup.merge(df_team2, on='match_id', how='left')\n",
    "    df_formation_lineup = pd.concat([df_team1_formation_lineup, df_team2_formation_lineup])\n",
    "    df_formation_lineup = split_dict_col(df_formation_lineup, 'lineup')\n",
    "    df_formation_lineup['bench'] = False\n",
    "    \n",
    "    # formation bench lineup\n",
    "    df_team1_formation_bench = list_dictionary_to_df(df_match, 'team1_formation_bench', 'lineup',\n",
    "                                                     'lineup_id', 'match_id')\n",
    "    df_team2_formation_bench = list_dictionary_to_df(df_match, 'team2_formation_bench', 'lineup',\n",
    "                                                     'lineup_id', 'match_id')\n",
    "    df_team1_formation_bench = df_team1_formation_bench.merge(df_team1, on='match_id', how='left')\n",
    "    df_team2_formation_bench = df_team2_formation_bench.merge(df_team2, on='match_id', how='left')\n",
    "    df_formation_bench = pd.concat([df_team1_formation_bench, df_team2_formation_bench])\n",
    "    df_formation_bench = split_dict_col(df_formation_bench, 'lineup')\n",
    "    df_formation_bench['bench'] = True\n",
    "    \n",
    "    # combine lineup from bench/ not from bench\n",
    "    df_formation = pd.concat([df_formation_lineup, df_formation_bench])\n",
    "        \n",
    "    df_formation.rename({'lineup_playerId': 'player_id', 'lineup_ownGoals': 'ownGoals',\n",
    "                         'lineup_redCards': 'redCards', 'lineup_goals': 'goals', 'lineup_yellowCards': 'yellowCards'},\n",
    "                        axis=1, inplace=True)\n",
    "    \n",
    "    # fix an error where the goalkeeper (Hitz) isn't starting (Jakob is in error): Borussia Dortmund vs Augsburg 2018-02-26\n",
    "    df_formation.loc[(df_formation.match_id == 2516947) & (df_formation.player_id == 14914), 'bench'] = False\n",
    "    df_formation.loc[(df_formation.match_id == 2516947) & (df_formation.player_id == 391449), 'bench'] = True\n",
    "    \n",
    "    # get a subsitutions dataframe\n",
    "    df_team1_formation_substitutions = list_dictionary_to_df(df_match, 'team1_formation_substitutions',\n",
    "                                                              'lineup', 'sub_id', 'match_id')\n",
    "    df_team2_formation_substitutions = list_dictionary_to_df(df_match, 'team2_formation_substitutions', \n",
    "                                                              'lineup', 'sub_id', 'match_id')\n",
    "    df_team1_formation_substitutions = df_team1_formation_substitutions.merge(df_team1, on='match_id', how='left')\n",
    "    df_team2_formation_substitutions = df_team2_formation_substitutions.merge(df_team2, on='match_id', how='left')\n",
    "    df_formation_substitutions = pd.concat([df_team1_formation_substitutions, df_team2_formation_substitutions])\n",
    "    df_formation_substitutions = df_formation_substitutions[df_formation_substitutions.lineup != 'null'].copy()\n",
    "    df_formation_substitutions = split_dict_col(df_formation_substitutions, 'lineup')\n",
    "    df_formation_substitutions.rename({'id': 'match_id', 'lineup_playerIn': 'player_id_in',\n",
    "                                       'lineup_playerOut': 'player_id_out', 'lineup_minute': 'minute'},\n",
    "                                      axis=1, inplace=True)\n",
    "    \n",
    "    # drop columns\n",
    "    df_match.drop(['date', 'status', 'winner', 'referees', 'team_id_away', 'team_id_home',\n",
    "                   'team1_formation_bench', 'team1_formation_lineup', 'team1_formation_substitutions',\n",
    "                   'team2_formation_bench', 'team2_formation_lineup', 'team2_formation_substitutions',\n",
    "                   'team1_hasFormation', 'team2_hasFormation',\n",
    "                   'team1_score', 'team1_scoreP', 'team1_scoreHT', 'team1_scoreET',\n",
    "                   'team2_score', 'team2_scoreP', 'team2_scoreHT', 'team2_scoreET',\n",
    "                   'teamsData', 'team1_teamId', 'team2_teamId', 'team2_side',\n",
    "                   'team1_side', 'team1_coachId', 'team2_coachId'], axis=1, inplace=True)\n",
    "    \n",
    "    save_path = os.path.join(DATA_FOLDER, 'match_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_match.to_parquet(save_path)\n",
    "    \n",
    "    save_path = os.path.join(DATA_FOLDER, 'formation_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_formation.to_parquet(save_path)\n",
    "    \n",
    "    save_path = os.path.join(DATA_FOLDER, 'substitution_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_formation_substitutions.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get matches as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_files = glob.glob(os.path.join(DATA_FOLDER, 'match_raw', '*.parquet'))\n",
    "df_match = pd.concat([pd.read_parquet(file) for file in match_files])\n",
    "df_match.to_parquet(os.path.join(DATA_FOLDER, 'match.parquet'))\n",
    "df_match.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the formation as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_FOLDER, 'formation_raw', '*.parquet'))\n",
    "df_formation = pd.concat([pd.read_parquet(file) for file in files])\n",
    "df_formation.to_parquet(os.path.join(DATA_FOLDER, 'formation.parquet'))\n",
    "df_formation.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the substitution as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(DATA_FOLDER, 'substitution_raw', '*.parquet'))\n",
    "df_substitution = pd.concat([pd.read_parquet(file) for file in files])\n",
    "df_substitution.to_parquet(os.path.join(DATA_FOLDER, 'substitution.parquet'))\n",
    "df_substitution.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dataframe with the tag description/ arrays with tag types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag = pd.read_csv(os.path.join(DATA_FOLDER, 'raw', 'tags2name.csv'))\n",
    "df_tag['Description'] = df_tag.Description.str.lower().str.replace(':', '').str.replace(' ', '_').str.replace('/', '_')\n",
    "position_tags = df_tag.loc[df_tag.Description.str[:8] == 'position', 'Tag'].values\n",
    "other_tags = df_tag.loc[df_tag.Description.str[:8] != 'position', 'Description'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through event files and save as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of event files\n",
    "events_list = glob.glob(os.path.join(DATA_FOLDER, 'raw', 'events*.json'))\n",
    "\n",
    "# loop through event files as save as seperate parquet files\n",
    "for file in events_list:\n",
    "    \n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    # load as dataframe\n",
    "    df_event = pd.read_json(file, encoding='unicode-escape')\n",
    "    \n",
    "    # split start and end positions\n",
    "    split_location_cols(df_event, 'positions', ['start', 'end'])\n",
    "    \n",
    "    # create seperate columns for the x/y coordinates\n",
    "    for col in ['start', 'end']:\n",
    "        df_event = split_dict_col(df_event, col)\n",
    "        \n",
    "    # set dodgy end coordinates to null\n",
    "    mask = df_event.eventName.isin(['Shot', 'Interruption', 'Offside'])\n",
    "    mask2 = df_event.subEventName.isin(['Free kick shot', 'Hand foul', 'Late card foul',\n",
    "                                        'Out of game foul', 'Protest',\n",
    "                                        'Simulation', 'Time lost foul', 'Violent Foul'])\n",
    "    df_event.loc[mask | mask2, 'end_x'] = np.nan\n",
    "    df_event.loc[mask | mask2, 'end_y'] = np.nan\n",
    "    \n",
    "    # wyscout has some dodgy end_y/ end_x near the corners. Convert to np.nan\n",
    "    mask_dodgy_end = (((df_event.end_y == 100) & (df_event.end_x == 100)) | \n",
    "                      ((df_event.end_x == 0) & (df_event.end_y == 0)))\n",
    "    df_event.loc[mask_dodgy_end, 'end_y'] = np.nan\n",
    "    df_event.loc[mask_dodgy_end, 'end_x'] = np.nan\n",
    "    \n",
    "    # set dodgy start coordinates to null\n",
    "    df_event.loc[df_event.eventName.isin(['Save attempt', 'Goalkeeper leaving line']), 'start_x'] = np.nan\n",
    "    df_event.loc[df_event.eventName.isin(['Save attempt', 'Goalkeeper leaving line']), 'start_y'] = np.nan\n",
    "    \n",
    "    # fix start coordinates for goal kicks\n",
    "    df_event.loc[df_event.subEventName == 'Goal kick', 'start_x'] = 6.\n",
    "    df_event.loc[df_event.subEventName == 'Goal kick', 'start_y'] = 50.\n",
    "    \n",
    "    # create a seperate column for each tag in the dictionary\n",
    "    df_new = pd.DataFrame(df_event['tags'].tolist(), index=df_event.index)\n",
    "    for tag in df_new.columns:\n",
    "        df_new.loc[df_new[tag].notnull(), tag] = df_new.loc[df_new[tag].notnull(), tag].apply(lambda x: x['id'])\n",
    "        \n",
    "    # summarise tag id columns into boolean columns for each tag and a string column for position \n",
    "    cols_to_drop = df_new.columns\n",
    "    for i, row in df_tag.iterrows():\n",
    "        if row['Tag'] not in position_tags:\n",
    "            df_new.loc[(df_new == row['Tag']).any(axis=1), row['Description']] = True\n",
    "        else:\n",
    "            df_new.loc[(df_new == row['Tag']).any(axis=1), 'position'] = row['Description']\n",
    "            \n",
    "    # remove 'position' and '_' from text in the position column\n",
    "    df_new['position'] = df_new.position.str[9:].str.replace('_', ' ')\n",
    "    df_new.loc[df_new['position'].isnull(), 'position'] = None\n",
    "    \n",
    "    # replace missing with False for boolean columns\n",
    "    df_new[other_tags] = df_new[other_tags].replace({np.nan: False})\n",
    "    \n",
    "    # drop tag id columns\n",
    "    df_new.drop(cols_to_drop, axis=1, inplace=True)                                               \n",
    "                                        \n",
    "    # add tags to the dataset\n",
    "    df_event = pd.concat([df_event, df_new], axis=1)\n",
    "    \n",
    "    # drop tag column\n",
    "    df_event.drop('tags', axis=1, inplace=True)\n",
    "    \n",
    "    # deal with blank subEventId\n",
    "    df_event.loc[df_event.subEventId=='', 'subEventId'] = None\n",
    "    df_event['subEventId'] = df_event['subEventId'].astype(np.float32)\n",
    "    \n",
    "    # rename columns for consistency with other datasets\n",
    "    df_event.rename({'playerId': 'player_id',\n",
    "                     'start_y': 'y',\n",
    "                     'start_x': 'x',\n",
    "                     'matchId': 'match_id',\n",
    "                     'teamId': 'team_id',}, axis=1, inplace=True)\n",
    "    \n",
    "    # save to parquet\n",
    "    save_path = os.path.join(DATA_FOLDER, 'event_raw', f'{os.path.basename(file)[:-4]}parquet')\n",
    "    df_event.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get events as a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = glob.glob(os.path.join(DATA_FOLDER, 'event_raw', '*.parquet'))\n",
    "df_event = pd.concat([pd.read_parquet(file) for file in event_files])\n",
    "df_event.sort_values(['match_id', 'matchPeriod', 'eventSec'], inplace=True)\n",
    "df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "df_event.info(verbose=True, show_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
