{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis for this thesis was run from commit:\n",
    "https://github.com/statsbomb/open-data/commit/20863db06d85306bd56f122a67fb7d03f2d15b70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the StatsBomb json files and turns them into parquet files. These are extremely fast to load so good for this prototyping kind of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplsoccer.statsbomb as sbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these paths/ parameters\n",
    "You will need to change these paths/ parameters depending on where the StatsBomb open-data is located, how and where you want to save the resulting data, and if you only want the new files to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data folder is one folder down in the directory. To change if run elsewhere\n",
    "STATSBOMB_DATA = os.path.join('..', '..', '..', 'open-data','data')\n",
    "# save files in folder in current directory. To change if want to save elsewhere\n",
    "DATA_FOLDER = os.path.join('..', '..', 'data', 'statsbomb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_links = glob.glob(os.path.join(STATSBOMB_DATA, 'events', '**', '*.json'),recursive=True)\n",
    "lineup_links = glob.glob(os.path.join(STATSBOMB_DATA, 'lineups', '**', '*.json'),recursive=True)\n",
    "match_links = glob.glob(os.path.join(STATSBOMB_DATA, 'matches', '**', '*.json'),recursive=True)\n",
    "competition_path = os.path.join(STATSBOMB_DATA, 'competitions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure\n",
    "for folder in ['event_raw', 'related_event_raw', 'freeze_frame_raw', 'tactic_raw', 'lineup_raw']:\n",
    "    path = os.path.join(DATA_FOLDER, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the competition datam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competition = sbapi.read_competition(competition_path, warn=False)\n",
    "# note there is a slight loss of data quality with timestamps, but these aren't relevant for analysis\n",
    "# pandas has nanoseconds, which aren't supported in parquet (supports milliseconds)\n",
    "df_competition.to_parquet(os.path.join(DATA_FOLDER, 'competition.parquet'), allow_truncated_timestamps=True)\n",
    "df_competition.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dfs = [sbapi.read_match(file, warn=False) for file in match_links]\n",
    "df_match = pd.concat(match_dfs)\n",
    "# again there is a slight loss of quality when saving timestamps, but only relevant for last_updated\n",
    "df_match.to_parquet(os.path.join(DATA_FOLDER, 'match.parquet'), allow_truncated_timestamps=True)\n",
    "df_match.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the lineup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINEUP_FOLDER = os.path.join(DATA_FOLDER, 'lineup_raw')\n",
    "# loop through the links and store as parquet files - small and fast files\n",
    "for file in lineup_links:\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    \n",
    "    # version that only loads new files\n",
    "    # if not os.path.isfile(os.path.join(LINEUP_FOLDER, save_path)):\n",
    "    #    try:\n",
    "    #        print('Trying:', file)\n",
    "    #        df_lineup = sbapi.read_lineup(file, warn=False)\n",
    "    #        df_lineup.to_parquet(os.path.join(LINEUP_FOLDER, save_path))\n",
    "    #    except:\n",
    "    #        print('Skipping:', file)\n",
    "    #        pass\n",
    "    \n",
    "    # version that loads all files\n",
    "    try:\n",
    "        print('Trying:', file)\n",
    "        df_lineup = sbapi.read_lineup(file, warn=False)\n",
    "        df_lineup.to_parquet(os.path.join(LINEUP_FOLDER, save_path))\n",
    "    except:\n",
    "        print('Skipping:', file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineup_files = glob.glob(os.path.join(LINEUP_FOLDER, '*.parquet'))\n",
    "df_lineup = pd.concat([pd.read_parquet(file) for file in lineup_files])\n",
    "# replace some ids that appear to be duplicated. Then de-duplicate\n",
    "df_lineup.player_id.replace({18103: 38522,  # Dietmar Hamann\n",
    "                             17275: 4656,  # Hannah Jayne Blundell\n",
    "                             17524: 4655,  # Jennifer Beattie\n",
    "                             10172: 4644,  # Jill Scott\n",
    "                             4634: 5088,  # Crystal Dunn\n",
    "                             }, inplace=True)\n",
    "df_lineup.to_parquet(os.path.join(DATA_FOLDER, 'lineup.parquet'))\n",
    "df_lineup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through the links and store as parquet files - small and fast files\n",
    "for file in event_links:\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    \n",
    "    # version that only loads new files\n",
    "    #if not os.path.isfile(os.path.join(DATA_FOLDER, 'event_raw', save_path)):\n",
    "    #    try:\n",
    "    #        print('Trying:', file)\n",
    "    #        dict_event = sbapi.read_event(file, warn=False)\n",
    "            # save to parquet files\n",
    "            # using the dictionary key to access the dataframes from the dictionary\n",
    "    #        dict_event['event'].to_parquet(os.path.join(DATA_FOLDER, 'event_raw', save_path))\n",
    "    #        dict_event['related_event'].to_parquet(os.path.join(DATA_FOLDER, 'related_event_raw', save_path))\n",
    "    #        dict_event['shot_freeze_frame'].to_parquet(os.path.join(DATA_FOLDER, 'freeze_frame_raw', save_path))\n",
    "    #        dict_event['tactics_lineup'].to_parquet(os.path.join(DATA_FOLDER, 'tactic_raw', save_path))\n",
    "    #    except:\n",
    "    #        print('Skipping:', file)\n",
    "    #        pass\n",
    "        \n",
    "    # version that loads all files\n",
    "    try:\n",
    "        print('Trying:', file)\n",
    "        dict_event = sbapi.read_event(file, warn=False)\n",
    "        # save to parquet files\n",
    "        # using the dictionary key to access the dataframes from the dictionary\n",
    "        dict_event['event'].to_parquet(os.path.join(DATA_FOLDER, 'event_raw', save_path))\n",
    "        dict_event['related_event'].to_parquet(os.path.join(DATA_FOLDER, 'related_event_raw', save_path))\n",
    "        dict_event['shot_freeze_frame'].to_parquet(os.path.join(DATA_FOLDER, 'freeze_frame_raw', save_path))\n",
    "        dict_event['tactics_lineup'].to_parquet(os.path.join(DATA_FOLDER, 'tactic_raw', save_path))\n",
    "    except:\n",
    "        print('Skipping:', file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "event_files = glob.glob(os.path.join(DATA_FOLDER, 'event_raw', '*.parquet'))\n",
    "df_event = pd.concat([pd.read_parquet(file) for file in event_files])\n",
    "df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "df_event.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe shot freeze frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_files = glob.glob(os.path.join(DATA_FOLDER, 'freeze_frame_raw', '*.parquet'))\n",
    "df_freeze = pd.concat([pd.read_parquet(file) for file in freeze_files])\n",
    "df_freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze.parquet'))\n",
    "df_freeze.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactic_files = glob.glob(os.path.join(DATA_FOLDER, 'tactic_raw', '*.parquet'))\n",
    "df_tactic = pd.concat([pd.read_parquet(file) for file in tactic_files])\n",
    "df_tactic.to_parquet(os.path.join(DATA_FOLDER, 'tactic.parquet'))\n",
    "df_tactic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe related events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_files = glob.glob(os.path.join(DATA_FOLDER, 'related_event_raw', '*.parquet'))\n",
    "df_related = pd.concat([pd.read_parquet(file) for file in related_files])\n",
    "df_related.to_parquet(os.path.join(DATA_FOLDER, 'related.parquet'))\n",
    "df_related.info(verbose=True, null_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
