{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis for this thesis was run from commit 87a5f02d7f526c4fe92909790999da5f26166328:\n",
    "https://github.com/statsbomb/open-data/commit/87a5f02d7f526c4fe92909790999da5f26166328\n",
    "\n",
    "From this commit, the dataframes have the following number of entries:\n",
    "- df_competition: 36 entries\n",
    "- df_match: 844 entries\n",
    "- df_lineup: 25172 entries\n",
    "- df_event: 3062353 entries\n",
    "- df_freeze: 267024 entries\n",
    "- df_tactic: 35335 entries\n",
    "- df_related: 5953632 entrie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the StatsBomb json files and turns them into parquet files. These are extremely fast to load so good for this prototyping kind of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplsoccer.statsbomb as sbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these paths/ parameters\n",
    "You will need to change these paths/ parameters depending on where the StatsBomb open-data is located, how and where you want to save the resulting data, and if you only want the new files to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data folder is one folder down in the directory. To change if run elsewhere\n",
    "STATSBOMB_DATA = os.path.join('..', '..', '..', 'open-data','data')\n",
    "# save files in folder in current directory. To change if want to save elsewhere\n",
    "DATA_FOLDER = os.path.join('..', '..', 'data', 'statsbomb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_links = glob.glob(os.path.join(STATSBOMB_DATA, 'events', '**', '*.json'),recursive=True)\n",
    "lineup_links = glob.glob(os.path.join(STATSBOMB_DATA, 'lineups', '**', '*.json'),recursive=True)\n",
    "match_links = glob.glob(os.path.join(STATSBOMB_DATA, 'matches', '**', '*.json'),recursive=True)\n",
    "competition_path = os.path.join(STATSBOMB_DATA, 'competitions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure\n",
    "for folder in ['event_raw', 'related_raw', 'freeze_raw', 'tactic_raw', 'lineup_raw']:\n",
    "    path = os.path.join(DATA_FOLDER, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competition = sbapi.read_competition(competition_path, warn=False)\n",
    "# note there is a slight loss of data quality with timestamps, but these aren't relevant for analysis\n",
    "# pandas has nanoseconds, which aren't supported in parquet (supports milliseconds)\n",
    "df_competition.to_parquet(os.path.join(DATA_FOLDER, 'competition.parquet'), allow_truncated_timestamps=True)\n",
    "df_competition.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep a copy of the match dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_path = os.path.join(DATA_FOLDER, 'match.parquet')\n",
    "if os.path.exists(match_path):\n",
    "    df_match_copy = pd.read_parquet(match_path).copy()\n",
    "    update_files = True\n",
    "else:\n",
    "    update_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dfs = [sbapi.read_match(file, warn=False) for file in match_links]\n",
    "df_match = pd.concat(match_dfs)\n",
    "# again there is a slight loss of quality when saving timestamps, but only relevant for last_updated\n",
    "df_match.to_parquet(os.path.join(DATA_FOLDER, 'match.parquet'), allow_truncated_timestamps=True)\n",
    "df_match.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check which games have been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_files == True:\n",
    "    df_match_copy = df_match[['match_id', 'last_updated']].merge(df_match_copy[['match_id', 'last_updated']],\n",
    "                                                                 how='left', suffixes=['', '_old'], on='match_id')\n",
    "    df_match_copy = df_match_copy[df_match_copy.last_updated.dt.floor('ms') != df_match_copy.last_updated_old].copy()\n",
    "    to_update = df_match_copy.match_id.unique()\n",
    "    \n",
    "    # get array of event links to update - based on whether they have been updated in the match json\n",
    "    event_link_ids = np.array([int(os.path.splitext(os.path.basename(link))[0]) for link in event_links])\n",
    "    event_to_update = [link in to_update for link in event_link_ids]\n",
    "    event_links = np.array(event_links)[event_to_update]\n",
    "    \n",
    "    # get array of lineup links to update - based on whether they have been updated in the match jsons\n",
    "    lineup_link_ids = np.array([int(os.path.splitext(os.path.basename(link))[0]) for link in lineup_links])\n",
    "    lineup_to_update = [link in to_update for link in lineup_link_ids]\n",
    "    lineup_links = np.array(lineup_links)[lineup_to_update]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the lineup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINEUP_FOLDER = os.path.join(DATA_FOLDER, 'lineup_raw')\n",
    "# loop through all the changed links and store as parquet files - small and fast files\n",
    "for file in lineup_links:\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    try:\n",
    "        print('Trying:', file)\n",
    "        df_lineup = sbapi.read_lineup(file, warn=False)\n",
    "        df_lineup.to_parquet(os.path.join(LINEUP_FOLDER, save_path))\n",
    "    except:\n",
    "        print('Skipping:', file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# players with split ids\n",
    "to_replace = {18103: 38522,  # Dietmar Hamann\n",
    "              17275: 4656,  # Hannah Jayne Blundell\n",
    "              17524: 4655,  # Jennifer Beattie\n",
    "              10172: 4644,  # Jill Scott\n",
    "              4634: 5088,  # Crystal Dunn\n",
    "              4639: 10395}  # Maren Mjelde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lineup_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    lineup_files = glob.glob(os.path.join(LINEUP_FOLDER, '*.parquet'))\n",
    "    df_lineup = pd.concat([pd.read_parquet(file) for file in lineup_files])\n",
    "    # replace some ids that appear to be split\n",
    "    df_lineup.player_id.replace(to_replace, inplace=True)\n",
    "    df_lineup.to_parquet(os.path.join(DATA_FOLDER, 'lineup.parquet'))\n",
    "    df_lineup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through all the changed links and store as parquet files - small and fast files\n",
    "for file in event_links:\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    try:\n",
    "        print('Trying:', file)\n",
    "        dict_event = sbapi.read_event(file, warn=False)\n",
    "        # save to parquet files\n",
    "        # using the dictionary key to access the dataframes from the dictionary\n",
    "        dict_event['event'].to_parquet(os.path.join(DATA_FOLDER, 'event_raw', save_path))\n",
    "        dict_event['related_event'].to_parquet(os.path.join(DATA_FOLDER, 'related_raw', save_path))\n",
    "        dict_event['shot_freeze_frame'].to_parquet(os.path.join(DATA_FOLDER, 'freeze_raw', save_path))\n",
    "        dict_event['tactics_lineup'].to_parquet(os.path.join(DATA_FOLDER, 'tactic_raw', save_path))\n",
    "    except:\n",
    "        print('Skipping:', file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If updating the event dataframe get a list of ids to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = glob.glob(os.path.join(DATA_FOLDER, 'event_raw', '*.parquet'))\n",
    "if update_files:\n",
    "    event_files_id = [int(os.path.splitext(os.path.basename(file))[0]) for file in event_files]\n",
    "    ids_to_update = [int(os.path.splitext(os.path.basename(link))[0]) for link in event_links]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load old dataframe and combine with updated parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(folder, file_type, ids_to_update):\n",
    "    # get a list of parquet files to add to the old dataframe\n",
    "    files = glob.glob(os.path.join(folder, f'{file_type}_raw', '*.parquet'))\n",
    "    files_id = [int(os.path.splitext(os.path.basename(file))[0]) for file in files]\n",
    "    mask_update = [match_id in ids_to_update for match_id in files_id]\n",
    "    files = np.array(files)[mask_update]\n",
    "    # load the old dataframe, filter out changed matches and add the new parquet files\n",
    "    df_old = pd.read_parquet(os.path.join(folder, f'{file_type}.parquet'))\n",
    "    df_old = df_old[~df_old.match_id.isin(ids_to_update)]\n",
    "    df_new = pd.concat([pd.read_parquet(file) for file in files])\n",
    "    df_old = pd.concat([df_old, df_new])\n",
    "    return df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if update_files:\n",
    "        df_event = update(DATA_FOLDER, 'event', ids_to_update)\n",
    "        df_event.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "        df_event.info(verbose=True, null_counts=True)        \n",
    "    else:\n",
    "        df_event = pd.concat([pd.read_parquet(file) for file in event_files])\n",
    "        df_event.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "        df_event.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe shot freeze frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if update_files:\n",
    "        df_freeze = update(DATA_FOLDER, 'freeze', ids_to_update)\n",
    "        df_freeze.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze.parquet'))\n",
    "        df_freeze.info(verbose=True, null_counts=True)      \n",
    "    else:\n",
    "        freeze_files = glob.glob(os.path.join(DATA_FOLDER, 'freeze_raw', '*.parquet'))\n",
    "        df_freeze = pd.concat([pd.read_parquet(file) for file in freeze_files])\n",
    "        df_freeze.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze.parquet'))\n",
    "        df_freeze.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if update_files:\n",
    "        df_tactic = update(DATA_FOLDER, 'tactic', ids_to_update)\n",
    "        df_tactic.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_tactic.to_parquet(os.path.join(DATA_FOLDER, 'tactic.parquet'))\n",
    "        df_tactic.info(verbose=True, null_counts=True)      \n",
    "    else:\n",
    "        tactic_files = glob.glob(os.path.join(DATA_FOLDER, 'tactic_raw', '*.parquet'))\n",
    "        df_tactic = pd.concat([pd.read_parquet(file) for file in tactic_files])\n",
    "        df_tactic.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_tactic.to_parquet(os.path.join(DATA_FOLDER, 'tactic.parquet'))\n",
    "        df_tactic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe related events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if update_files:\n",
    "        df_related = update(DATA_FOLDER, 'related', ids_to_update)\n",
    "        df_related.to_parquet(os.path.join(DATA_FOLDER, 'related.parquet'))\n",
    "        df_related.info(verbose=True, null_counts=True)     \n",
    "    else:\n",
    "        related_files = glob.glob(os.path.join(DATA_FOLDER, 'related_raw', '*.parquet'))\n",
    "        df_related = pd.concat([pd.read_parquet(file) for file in related_files])\n",
    "        df_related.to_parquet(os.path.join(DATA_FOLDER, 'related.parquet'))\n",
    "        df_related.info(verbose=True, null_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
